# -*- coding: utf-8 -*-
"""Diamonds.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1QmJuzbi7Hh-9hdrKGvX05jmgg1Fm3ZUi
"""

# Commented out IPython magic to ensure Python compatibility.
import pandas as pd 
import numpy as np
import matplotlib.pyplot as plt
import seaborn as sns
# %matplotlib inline
from pandas.plotting import scatter_matrix
from sklearn.preprocessing import OneHotEncoder,OrdinalEncoder
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.compose import ColumnTransformer
from sklearn.compose import make_column_transformer
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error,r2_score
from sklearn.tree import DecisionTreeRegressor 
from sklearn.model_selection import cross_val_score
from sklearn.neighbors import KNeighborsRegressor
from sklearn.model_selection import GridSearchCV

"""# **1- Get the Data**"""

from google.colab import files

uploaded = files.upload()

diamonds= pd.read_csv('diamonds.csv')
diamonds.head()

diamond=diamonds.drop('Unnamed: 0',axis=1)
diamond.head()

"""# **2- Discover and visualize the data**

# **A- Data discovery**
"""

diamond.info()

diamond.describe()

"""min in (x,y,z) equal 0 impossible so there is a mistaque. let's show the total of o in these attributes.








"""

(diamond==0).sum()

"""let's take a look about categorical varaibles"""

diamonds['cut'].value_counts()

diamonds['color'].value_counts()

diamonds['clarity'].value_counts()

"""# B- Data visualization"""

diamond.hist(bins=50,figsize=(12,7))

attributes = ["carat", "depth", "price", "table"]
scatter_matrix(diamond[attributes], figsize=(10,7))

"""the relation between price and carat"""

diamond.plot(kind='scatter' ,x='price',y='carat')

diamond.plot(kind='scatter' ,x='price',y='depth')

diamond.plot(kind='scatter' ,x='price',y='table')

"""# correlation between numirical attributes"""

diamond.corr()

plt.figure(figsize=(10,8))
sns.heatmap(data=diamond.corr(),annot=True)

sns.pairplot(diamond)

sns.factorplot(x='cut', data=diamond, kind='count' )

sns.factorplot(x='color', data=diamond, kind='count' )

sns.factorplot(x='clarity', data=diamond, kind='count'  )

sns.boxplot(data=diamond,x='color',y='price')

sns.boxplot(data=diamond,x='cut',y='price')

sns.boxplot(data=diamond,x='clarity',y='price')

"""# **3- Prepare the data**

# A- Data Cleaning
"""

diamond.isnull().sum()

"""Great no missing values  but like see  (x,y,z) have null values let's drop it.

"""

diamond=diamond.drop(diamond[diamond['x']==0].index)
diamond=diamond.drop(diamond[diamond['y']==0].index)
diamond=diamond.drop(diamond[diamond['z']==0].index)
diamond.shape

"""creat a new variable call volume"""

diamond['volume']=diamond['x']*diamond['y']*diamond['z']
diamond.head()

diamond.corr()

plt.figure(figsize=(15,16))
attributes=['carat','depth', 'table','price','volume','color']
sns.pairplot(diamond,hue='color')

"""# Delete the variables x,y,z"""

attribut=['x','y','z']
diamond=diamond.drop(diamond[attribut],axis=1)
diamond.head()

"""# B-Handling Text and Categorical Attributes"""

print(diamond["color"].unique())

diamond_col=diamond[['color']]
ohe=OneHotEncoder()
ohe_col=ohe.fit_transform(diamond_col)

ohe_col.toarray()

print(diamond["clarity"].unique())

diamond_clar=diamond[['clarity']]
ohe=OneHotEncoder()
ohe_clar=ohe.fit_transform(diamond_clar)

print(diamond["cut"].unique())

diamond_cut=diamond[['cut']]
enc = OrdinalEncoder(categories=[['Ideal','Premium' ,'Very Good','Good','Fair']])
enc_cut= enc.fit_transform(diamond_cut)
enc_cut

"""# C- Feature Scaling"""

attribut=["color","clarity","cut"]
diamond_num = diamond.drop(attribut, axis=1)

"""To scale numerical values we created diamond_num that contain numerical values.
Now use sklearn StandardScaler to fit and transform diamond_num.
"""

scaler= StandardScaler()
scaler_diamond=scaler.fit_transform(diamond_num)
scaler_diamond

"""# Create a Test Set and Train Set"""

train_set, test_set = train_test_split(diamond, test_size=0.2, random_state=42)
diamond = train_set.drop("price", axis=1)
diamond_labels = train_set["price"].copy()

"""make transform for all dataset"""

attribut=["color","clarity","cut"]
diamond_num = diamond.drop(attribut, axis=1)
num_attribs = list(diamond_num)
column_trans = make_column_transformer((OrdinalEncoder(categories=[['D','E','F','G','H','I','J']]),['color'])
,(OrdinalEncoder(categories=[['IF','VVS1', 'VVS2', 'VS1', 'VS2', 'SI1', 'SI2','I1']]),['clarity']),
                                      (OrdinalEncoder(categories=[['Fair', 'Good', 'Very Good', 'Premium', 'Ideal']]), ['cut']),
                                      (StandardScaler(),num_attribs) , remainder='passthrough')

diamond_prepared=column_trans.fit_transform(diamond)
diamond_prepared

"""# **3- Select and Train a Model**
# Let’s first train a LinearRegression model
"""

lin_reg =LinearRegression()
lin_reg.fit(diamond_prepared,diamond_labels)

"""# measure this regression model’s RMSE on the whole training set
sing Scikit-Learn’s mean_squared_error() function:
"""

diamond_prediction= lin_reg.predict(diamond_prepared)
lin_mse=mean_squared_error(diamond_prediction,diamond_labels)
lin_rmse=np.sqrt(lin_mse)
lin_rmse

"""There is underfiting

# Let’s train a Decision Tree Regressor model
more powerful model
"""

tree_reg= DecisionTreeRegressor()
tree_reg.fit(diamond_prepared,diamond_labels)

"""measure this decision tree model’s RMSE on the whole training set"""

diamond_prodictions=tree_reg.predict(diamond_prepared)
lin_mse=mean_squared_error(diamond_prodictions,diamond_labels)
lin_rmse=np.sqrt(lin_mse)
lin_rmse

"""it seem good but let's with cross-validation

# Evaluation Using Cross-Validation

cross_validation with tree decision
"""

scores=cross_val_score(tree_reg,diamond_prepared,diamond_labels,scoring="neg_mean_squared_error",cv=10)
tree_rmse_scores=np.sqrt(-scores)

def display(scores):
  print('scores:',scores)
  print('mean:',scores.mean())
  print('standard deviation:',scores.std())
display(tree_rmse_scores)

"""cross-validation with lenaire regression"""

lin_scores=cross_val_score(lin_reg,diamond_prepared,diamond_labels,scoring="neg_mean_squared_error",cv=10)
lin_rmse_scores=np.sqrt(-lin_scores)
display(lin_rmse_scores)

"""# Let’s train the RandomForestRegressor model."""

from sklearn.ensemble import RandomForestRegressor
forest_reg = RandomForestRegressor()
forest_reg.fit(diamond_prepared, diamond_labels)

diamond_prediction= forest_reg.predict(diamond_prepared)
lin_fmse=mean_squared_error(diamond_prediction,diamond_labels)
lin_frmse=np.sqrt(lin_fmse)
lin_frmse

"""cross-validation with  randomforest model"""

forest_scores=cross_val_score(forest_reg,diamond_prepared,diamond_labels,scoring="neg_mean_squared_error",cv=10)
forest_rmse_scores=np.sqrt(-forest_scores)
display(forest_rmse_scores)

"""# Let’s train the  KNeighborsRegressor model."""

reg_knn = KNeighborsRegressor()
reg_knn.fit( diamond_prepared,diamond_labels)

diamond_prediction= reg_knn.predict(diamond_prepared)
lin_fmse=mean_squared_error(diamond_prediction,diamond_labels)
lin_frmse=np.sqrt(lin_fmse)
lin_frmse

"""cross_validation KNeighborsRegressor."""

knn_scores = cross_val_score(reg_knn,diamond_prepared,diamond_labels,scoring="neg_mean_squared_error",cv=10)
knn_rmse_scores=np.sqrt(-knn_scores)
display(knn_rmse_scores)

"""# Let’s train one last model the AdaBoostRegressor."""

from sklearn.ensemble import AdaBoostRegressor
ad_reg = AdaBoostRegressor()
ad_reg.fit(diamond_prepared, diamond_labels)
diamond_prediction= reg_knn.predict(diamond_prepared)
lin_fmse=mean_squared_error(diamond_prediction,diamond_labels)
lin_frmse=np.sqrt(lin_fmse)
lin_frmse

"""cross_validation AdaBoostRegressor model.




"""

ad_reg_scores = cross_val_score(ad_reg,diamond_prepared,diamond_labels,scoring="neg_mean_squared_error",cv=10)
knn_rmse_scores=np.sqrt(-ad_reg_scores)
display(knn_rmse_scores)

"""#Fine-Tune Your Model
1- Grid Search
evaluate all the possible combinations of hyperparameter values for the RandomForestRegressor 
"""

param_grid = [
{'n_estimators': [3, 10, 30], 'max_features': [2, 4, 6, 8]},
{'bootstrap': [False], 'n_estimators': [3, 10], 'max_features': [2, 3, 4]},
]
forest_reg = RandomForestRegressor()
grid_search_for = GridSearchCV(forest_reg, param_grid, cv=5,
scoring='neg_mean_squared_error',
return_train_score=True)
grid_search_for.fit(diamond_prepared, diamond_labels)

cvres = grid_search_for.cv_results_
for mean_score, params in zip(cvres["mean_test_score"], cvres["params"]):
    print(np.sqrt(-mean_score), params)

"""#Now is the time to evaluate the final model on the test set.
Evaluate Your System on the Test Set
"""

final_model = grid_search_for.best_estimator_
X_test = test_set.drop("price", axis=1)
y_test = test_set["price"].copy()
X_test_prepared = column_trans.transform(X_test)

final_predictions = final_model.predict(X_test_prepared)
final_mse = mean_squared_error(y_test, final_predictions)
final_rmse = np.sqrt(final_mse)
final_rmse
r2 = r2_score(y_test, final_predictions)
r2